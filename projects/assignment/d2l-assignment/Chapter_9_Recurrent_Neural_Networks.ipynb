{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Recurrent Neural Networks\n",
    "\n",
    "This chapter introduces recurrent neural networks (RNNs), which are deep learning models that capture the dynamics of sequences via *recurrent* connections. RNNs can be thought of as feedforward neural networks where each layer's parameters are shared across time steps.\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Unlike feedforward networks, RNNs maintain a *hidden state* that acts as memory, allowing them to process sequences of variable length while keeping the number of parameters constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.1 Working with Sequences\n",
    "\n",
    "We focus on inputs that consist of an ordered list of feature vectors, where each is indexed by a time step. The key challenge is that the number of inputs varies depending on the sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Models\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Autoregressive models predict the next value based on previous observations. The key challenge is that the number of inputs grows with time, so we either (1) use a fixed-length window, or (2) maintain a summary hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We create synthetic data following the `sin` function with additive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(d2l.DataModule):\n",
    "    def __init__(self, batch_size=16, T=1000, num_train=600, tau=4):\n",
    "        self.save_hyperparameters()\n",
    "        self.time = d2l.arange(1, T + 1, dtype=d2l.float32)\n",
    "        self.x = d2l.sin(0.01 * self.time) + d2l.randn(T) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "d2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Data)\n",
    "def get_dataloader(self, train):\n",
    "    features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]\n",
    "    self.features = d2l.stack(features, 1)\n",
    "    self.labels = d2l.reshape(self.x[self.tau:], (-1, 1))\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader([self.features, self.labels], train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = d2l.LinearRegression(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=5)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: One-step-ahead prediction works well, but multi-step prediction degrades rapidly because errors accumulateâ€”each prediction feeds into the next, compounding mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onestep_preds = d2l.numpy(model(data.features))\n",
    "d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',\n",
    "         legend=['labels', '1-step preds'], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep_preds = d2l.zeros(data.T)\n",
    "multistep_preds[:] = data.x\n",
    "for i in range(data.num_train + data.tau, data.T):\n",
    "    multistep_preds[i] = model(\n",
    "        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))\n",
    "multistep_preds = d2l.numpy(multistep_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.plot([data.time[data.tau:], data.time[data.num_train+data.tau:]],\n",
    "         [onestep_preds, multistep_preds[data.num_train+data.tau:]], 'time',\n",
    "         'x', legend=['1-step preds', 'multistep preds'], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_step_pred(k):\n",
    "    features = []\n",
    "    for i in range(data.tau):\n",
    "        features.append(data.x[i : i+data.T-data.tau-k+1])\n",
    "    # The (i+tau)-th element stores the (i+1)-step-ahead predictions\n",
    "    for i in range(k):\n",
    "        preds = model(d2l.stack(features[i : i+data.tau], 1))\n",
    "        features.append(d2l.reshape(preds, -1))\n",
    "    return features[data.tau:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = (1, 4, 16, 64)\n",
    "preds = k_step_pred(steps[-1])\n",
    "d2l.plot(data.time[data.tau+steps[-1]-1:],\n",
    "         [d2l.numpy(preds[k-1]) for k in steps], 'time', 'x',\n",
    "         legend=[f'{k}-step preds' for k in steps], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.2 Converting Raw Text into Sequence Data\n",
    "\n",
    "Text preprocessing pipeline: (1) Load text as strings, (2) Split into tokens, (3) Build vocabulary mapping tokens to indices, (4) Convert text to numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeMachine(d2l.DataModule):  #@save\n",
    "    \"\"\"The Time Machine dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n",
    "                             '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "        with open(fname) as f:\n",
    "            return f.read()\n",
    "\n",
    "data = TimeMachine()\n",
    "raw_text = data._download()\n",
    "raw_text[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(TimeMachine)  #@save\n",
    "def _preprocess(self, text):\n",
    "    return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "text = data._preprocess(raw_text)\n",
    "text[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(TimeMachine)  #@save\n",
    "def _tokenize(self, text):\n",
    "    return list(text)\n",
    "\n",
    "tokens = data._tokenize(text)\n",
    "','.join(tokens[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: A vocabulary maps each unique token to a numerical index. Rare tokens are often replaced with a special `<unk>` token to keep the vocabulary manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)\n",
    "indices = vocab[tokens[:10]]\n",
    "print('indices:', indices)\n",
    "print('words:', vocab.to_tokens(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(TimeMachine)  #@save\n",
    "def build(self, raw_text, vocab=None):\n",
    "    tokens = self._tokenize(self._preprocess(raw_text))\n",
    "    if vocab is None: vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for token in tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = data.build(raw_text)\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Language Statistics\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Word frequencies follow Zipf's lawâ€”the frequency of a word is inversely proportional to its rank. This holds for unigrams, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "vocab = Vocab(words)\n",
    "vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [freq for token, freq in vocab.token_freqs]\n",
    "d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\n",
    "         xscale='log', yscale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]\n",
    "bigram_vocab = Vocab(bigram_tokens)\n",
    "bigram_vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tokens = ['--'.join(triple) for triple in zip(\n",
    "    words[:-2], words[1:-1], words[2:])]\n",
    "trigram_vocab = Vocab(trigram_tokens)\n",
    "trigram_vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
    "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
    "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
    "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
    "         legend=['unigram', 'bigram', 'trigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.3 Language Models\n",
    "\n",
    "The goal of language models is to estimate the joint probability of a sequence: $P(x_1, x_2, \\ldots, x_T)$.\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: We decompose the joint probability using the chain rule: $P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t | x_{t-1}, \\ldots, x_1)$. This allows autoregressive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Sequences\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: For training, we partition sequences into fixed-length subsequences. The target is simply the input shifted by one tokenâ€”we predict the next token given previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  #@save\n",
    "def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "    super(d2l.TimeMachine, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    corpus, self.vocab = self.build(self._download())\n",
    "    array = d2l.tensor([corpus[i:i+num_steps+1]\n",
    "                        for i in range(len(corpus)-num_steps)])\n",
    "    self.X, self.Y = array[:,:-1], array[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    idx = slice(0, self.num_train) if train else slice(\n",
    "        self.num_train, self.num_train + self.num_val)\n",
    "    return self.get_tensorloader([self.X, self.Y], train, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=2, num_steps=10)\n",
    "for X, Y in data.train_dataloader():\n",
    "    print('X:', X, '\\nY:', Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.4 Recurrent Neural Networks\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: RNNs use a *hidden state* $h_t$ to compress all sequence information up to time $t$. The hidden state is updated as: $h_t = f(x_t, h_{t-1})$. This allows handling arbitrarily long sequences with fixed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs with Hidden States\n",
    "\n",
    "The hidden state equation: $\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: The computation $\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ is equivalent to concatenating inputs and hidden state, then multiplying by a single weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, W_xh = d2l.randn(3, 1), d2l.randn(1, 4)\n",
    "H, W_hh = d2l.randn(3, 4), d2l.randn(4, 4)\n",
    "d2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.matmul(d2l.concat((X, H), 1), d2l.concat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.5 RNN Implementation from Scratch\n",
    "\n",
    "We implement an RNN from scratch and train it as a character-level language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(d2l.Module):  #@save\n",
    "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W_xh = nn.Parameter(\n",
    "            d2l.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.W_hh = nn.Parameter(\n",
    "            d2l.randn(num_hiddens, num_hiddens) * sigma)\n",
    "        self.b_h = nn.Parameter(d2l.zeros(num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNScratch)  #@save\n",
    "def forward(self, inputs, state=None):\n",
    "    if state is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        state = d2l.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          device=inputs.device)\n",
    "    else:\n",
    "        state, = state\n",
    "    outputs = []\n",
    "    for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
    "        state = d2l.tanh(d2l.matmul(X, self.W_xh) +\n",
    "                         d2l.matmul(state, self.W_hh) + self.b_h)\n",
    "        outputs.append(state)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n",
    "rnn = RNNScratch(num_inputs, num_hiddens)\n",
    "X = d2l.ones((num_steps, batch_size, num_inputs))\n",
    "outputs, state = rnn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len(a, n):  #@save\n",
    "    \"\"\"Check the length of a list.\"\"\"\n",
    "    assert len(a) == n, f'list\\'s length {len(a)} != expected length {n}'\n",
    "\n",
    "def check_shape(a, shape):  #@save\n",
    "    \"\"\"Check the shape of a tensor.\"\"\"\n",
    "    assert a.shape == shape, \\\n",
    "            f'tensor\\'s shape {a.shape} != expected shape {shape}'\n",
    "\n",
    "check_len(outputs, num_steps)\n",
    "check_shape(outputs[0], (batch_size, num_hiddens))\n",
    "check_shape(state, (batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-Based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLMScratch(d2l.Classifier):  #@save\n",
    "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            d2l.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
    "        self.b_q = nn.Parameter(d2l.zeros(self.vocab_size))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', d2l.exp(l), train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', d2l.exp(l), train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: We use one-hot encoding to represent tokens because treating token indices as numerical values would incorrectly imply ordering relationships between unrelated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch)  #@save\n",
    "def one_hot(self, X):\n",
    "    # Output shape: (num_steps, batch_size, vocab_size)\n",
    "    return F.one_hot(X.T, self.vocab_size).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming RNN Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch)  #@save\n",
    "def output_layer(self, rnn_outputs):\n",
    "    outputs = [d2l.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
    "    return d2l.stack(outputs, 1)\n",
    "\n",
    "@d2l.add_to_class(RNNLMScratch)  #@save\n",
    "def forward(self, X, state=None):\n",
    "    embs = self.one_hot(X)\n",
    "    rnn_outputs, _ = self.rnn(embs, state)\n",
    "    return self.output_layer(rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLMScratch(rnn, num_inputs)\n",
    "outputs = model(d2l.ones((batch_size, num_steps), dtype=d2l.int64))\n",
    "check_shape(outputs, (batch_size, num_steps, num_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: RNNs suffer from exploding gradients due to repeated matrix multiplications through time. Gradient clipping projects gradients onto a ball of radius Î¸: $\\mathbf{g} \\leftarrow \\min(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}) \\mathbf{g}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def clip_gradients(self, grad_clip_val, model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > grad_clip_val:\n",
    "        for param in params:\n",
    "            param.grad[:] *= grad_clip_val / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: During generation, we use a \"warm-up\" period where we feed the prefix to build up the hidden state, then generate one token at a time by feeding each prediction back as the next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch)  #@save\n",
    "def predict(self, prefix, num_preds, vocab, device=None):\n",
    "    state, outputs = None, [vocab[prefix[0]]]\n",
    "    for i in range(len(prefix) + num_preds - 1):\n",
    "        X = d2l.tensor([[outputs[-1]]], device=device)\n",
    "        embs = self.one_hot(X)\n",
    "        rnn_outputs, state = self.rnn(embs, state)\n",
    "        if i < len(prefix) - 1:  # Warm-up period\n",
    "            outputs.append(vocab[prefix[i + 1]])\n",
    "        else:  # Predict num_preds steps\n",
    "            Y = self.output_layer(rnn_outputs)\n",
    "            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.6 Concise Implementation of RNNs\n",
    "\n",
    "Using high-level APIs for faster, optimized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(d2l.Module):  #@save\n",
    "    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        return self.rnn(inputs, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(d2l.RNNLMScratch):  #@save\n",
    "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
    "    def init_params(self):\n",
    "        self.linear = nn.LazyLinear(self.vocab_size)\n",
    "\n",
    "    def output_layer(self, hiddens):\n",
    "        return d2l.swapaxes(self.linear(hiddens), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "model.predict('it has', 20, data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.7 Backpropagation Through Time\n",
    "\n",
    "Backpropagation through time (BPTT) is how we compute gradients in RNNs by unrolling the network across time steps.\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: The gradient $\\partial h_t / \\partial w_h$ involves a product of matrices across all time steps. If eigenvalues are < 1, gradients vanish; if > 1, gradients explode. This is why we need gradient clipping and why LSTMs/GRUs were invented.\n",
    "\n",
    "### Strategies for Computing Gradients:\n",
    "\n",
    "1. **Full Computation**: Compute the entire sumâ€”computationally expensive and numerically unstable\n",
    "2. **Truncated BPTT**: Only backpropagate through Ï„ time stepsâ€”practical and commonly used\n",
    "3. **Randomized Truncation**: Randomly truncate with correct expectationâ€”theoretically nice but not much better in practice\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Truncated BPTT acts as a regularizer, biasing the model toward shorter-range dependencies. This is often desirable in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key takeaways from Chapter 9:\n",
    "\n",
    "1. **Sequences require special handling**: The variable-length nature of sequences requires either fixed windows or hidden states to maintain context.\n",
    "\n",
    "2. **RNNs use recurrent connections**: Hidden state $h_t = f(x_t, h_{t-1})$ captures sequence history with constant parameters.\n",
    "\n",
    "3. **Language modeling is autoregressive**: Predict next token given previous tokens using chain rule decomposition.\n",
    "\n",
    "4. **Gradient clipping is essential**: Prevents exploding gradients that destabilize training.\n",
    "\n",
    "5. **Perplexity measures model quality**: $\\exp(-\\frac{1}{n}\\sum_t \\log P(x_t|x_{<t}))$ - lower is better.\n",
    "\n",
    "6. **Truncated BPTT is practical**: Full backpropagation is unstable; truncating provides a good tradeoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
