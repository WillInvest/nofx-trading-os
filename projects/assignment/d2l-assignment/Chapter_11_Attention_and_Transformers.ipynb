{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Attention Mechanisms and Transformers\n",
    "\n",
    "This notebook covers the attention mechanism and Transformer architecture, which have revolutionized deep learning since 2017.\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: The Transformer architecture, based on self-attention, has become the dominant model for NLP and is increasingly used in computer vision. Unlike RNNs, Transformers can process all positions in parallel and directly model relationships between any two positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Queries, Keys, and Values\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Attention is like a database query - given a query, we look up similar keys and return a weighted combination of their values. The key innovation is making this process differentiable via softmax normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\n",
    "                  cmap='Reds'):\n",
    "    \"\"\"Show heatmaps of matrices.\"\"\"\n",
    "    d2l.use_svg_display()\n",
    "    num_rows, num_cols, _, _ = matrices.shape\n",
    "    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,\n",
    "                                 sharex=True, sharey=True, squeeze=False)\n",
    "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n",
    "        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n",
    "            pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)\n",
    "            if i == num_rows - 1:\n",
    "                ax.set_xlabel(xlabel)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(ylabel)\n",
    "            if titles:\n",
    "                ax.set_title(titles[j])\n",
    "    fig.colorbar(pcm, ax=axes, shrink=0.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = d2l.reshape(d2l.eye(10), (1, 1, 10, 10))\n",
    "show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Attention Pooling by Similarity\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Nadaraya-Watson kernel regression is an early precursor of attention - it computes output as a weighted average of values, where weights depend on query-key similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "d2l.use_svg_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some kernels\n",
    "def gaussian(x):\n",
    "    return d2l.exp(-x**2 / 2)\n",
    "\n",
    "def boxcar(x):\n",
    "    return d2l.abs(x) < 1.0\n",
    "\n",
    "def constant(x):\n",
    "    return 1.0 + 0 * x\n",
    "\n",
    "def epanechikov(x):\n",
    "    return torch.max(1 - d2l.abs(x), torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))\n",
    "\n",
    "kernels = (gaussian, boxcar, constant, epanechikov)\n",
    "names = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')\n",
    "x = d2l.arange(-2.5, 2.5, 0.1)\n",
    "for kernel, name, ax in zip(kernels, names, axes):\n",
    "    ax.plot(d2l.numpy(x), d2l.numpy(kernel(x)))\n",
    "    ax.set_xlabel(name)\n",
    "\n",
    "d2l.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2 * d2l.sin(x) + x\n",
    "\n",
    "n = 40\n",
    "x_train, _ = torch.sort(d2l.rand(n) * 5)\n",
    "y_train = f(x_train) + d2l.randn(n)\n",
    "x_val = d2l.arange(0, 5, 0.1)\n",
    "y_val = f(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nadaraya_watson(x_train, y_train, x_val, kernel):\n",
    "    dists = d2l.reshape(x_train, (-1, 1)) - d2l.reshape(x_val, (1, -1))\n",
    "    # Each column/row corresponds to each query/key\n",
    "    k = d2l.astype(kernel(dists), d2l.float32)\n",
    "    # Normalization over keys for each query\n",
    "    attention_w = k / d2l.reduce_sum(k, 0)\n",
    "    y_hat = y_train@attention_w\n",
    "    return y_hat, attention_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x_train, y_train, x_val, y_val, kernels, names, attention=False):\n",
    "    fig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))\n",
    "    for kernel, name, ax in zip(kernels, names, axes):\n",
    "        y_hat, attention_w = nadaraya_watson(x_train, y_train, x_val, kernel)\n",
    "        if attention:\n",
    "            pcm = ax.imshow(d2l.numpy(attention_w), cmap='Reds')\n",
    "        else:\n",
    "            ax.plot(x_val, y_hat)\n",
    "            ax.plot(x_val, y_val, 'm--')\n",
    "            ax.plot(x_train, y_train, 'o', alpha=0.5);\n",
    "        ax.set_xlabel(name)\n",
    "        if not attention:\n",
    "            ax.legend(['y_hat', 'y'])\n",
    "    if attention:\n",
    "        fig.colorbar(pcm, ax=axes, shrink=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_train, y_train, x_val, y_val, kernels, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_train, y_train, x_val, y_val, kernels, names, attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = (0.1, 0.2, 0.5, 1)\n",
    "names = ['Sigma ' + str(sigma) for sigma in sigmas]\n",
    "\n",
    "def gaussian_with_width(sigma):\n",
    "    return (lambda x: d2l.exp(-x**2 / (2*sigma**2)))\n",
    "\n",
    "kernels = [gaussian_with_width(sigma) for sigma in sigmas]\n",
    "plot(x_train, y_train, x_val, y_val, kernels, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_train, y_train, x_val, y_val, kernels, names, attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Attention Scoring Functions\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Scaled dot-product attention divides by âˆšd to maintain stable gradients regardless of dimension. This is the core of modern Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):  #@save\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), d2l.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = d2l.ones((2, 3, 4))\n",
    "K = d2l.ones((2, 4, 6))\n",
    "d2l.check_shape(torch.bmm(Q, K), (2, 3, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):  #@save\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = d2l.normal(0, 1, (2, 1, 2))\n",
    "keys = d2l.normal(0, 1, (2, 10, 2))\n",
    "values = d2l.normal(0, 1, (2, 10, 4))\n",
    "valid_lens = d2l.tensor([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(d2l.reshape(attention.attention_weights, (1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):  #@save\n",
    "    \"\"\"Additive attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.w_v = nn.LazyLinear(1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
    "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
    "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of self.w_v, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
    "        # no. of queries, no. of key-value pairs)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = d2l.normal(0, 1, (2, 1, 20))\n",
    "\n",
    "attention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n",
    "attention.eval()\n",
    "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(d2l.reshape(attention.attention_weights, (1, 1, 2, 10)),\n",
    "                  xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 The Bahdanau Attention Mechanism\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Bahdanau attention allows the decoder to dynamically focus on different parts of the input at each decoding step, solving the information bottleneck problem of fixed-length context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(d2l.Decoder):  #@save\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "            dropout=dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(d2l.init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        # Shape of outputs: (num_steps, batch_size, num_hiddens).\n",
    "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n",
    "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # Shape of the output X: (num_steps, batch_size, embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            # Shape of query: (batch_size, 1, num_hiddens)\n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            # Shape of context: (batch_size, 1, num_hiddens)\n",
    "            context = self.attention(\n",
    "                query, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "            # Concatenate on the feature dimension\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        # After fully connected layer transformation, shape of outputs:\n",
    "        # (num_steps, batch_size, vocab_size)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
    "                                          enc_valid_lens]\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 7\n",
    "encoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\n",
    "                                  num_layers)\n",
    "X = d2l.zeros((batch_size, num_steps), dtype=torch.long)\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "output, state = decoder(X, state)\n",
    "d2l.check_shape(output, (batch_size, num_steps, vocab_size))\n",
    "d2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))\n",
    "d2l.check_shape(state[1][0], (batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.MTFraEng(batch_size=128)\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
    "encoder = d2l.Seq2SeqEncoder(\n",
    "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(\n",
    "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                    lr=0.005)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "preds, _ = model.predict_step(\n",
    "    data.build(engs, fras), d2l.try_gpu(), data.num_steps)\n",
    "for en, fr, p in zip(engs, fras, preds):\n",
    "    translation = []\n",
    "    for token in data.tgt_vocab.to_tokens(p):\n",
    "        if token == '<eos>':\n",
    "            break\n",
    "        translation.append(token)\n",
    "    print(f'{en} => {translation}, bleu,'\n",
    "          f'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dec_attention_weights = model.predict_step(\n",
    "    data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\n",
    "attention_weights = d2l.concat(\n",
    "    [step[0][0][0] for step in dec_attention_weights], 0)\n",
    "attention_weights = d2l.reshape(attention_weights, (1, 1, -1, data.num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus one to include the end-of-sequence token\n",
    "d2l.show_heatmaps(\n",
    "    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),\n",
    "    xlabel='Key positions', ylabel='Query positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Multi-Head Attention\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Multi-head attention runs multiple attention functions in parallel, each with different learned projections. This allows the model to jointly attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(d2l.Module):  #@save\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
    "            # times, then copy the next item, and so on\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
    "        # num_hiddens / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_qkv(self, X):\n",
    "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "    # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_output(self, X):\n",
    "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = d2l.tensor([3, 2])\n",
    "X = d2l.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = d2l.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Self-Attention and Positional Encoding\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Self-attention allows each token to attend to all other tokens, enabling O(1) maximum path length (vs O(n) for RNNs). Positional encoding adds sequence order information since self-attention is inherently position-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])\n",
    "X = d2l.ones((batch_size, num_queries, num_hiddens))\n",
    "d2l.check_shape(attention(X, X, X, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = d2l.zeros((1, max_len, num_hiddens))\n",
    "        X = d2l.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim, num_steps = 32, 60\n",
    "pos_encoding = PositionalEncoding(encoding_dim, 0)\n",
    "X = pos_encoding(d2l.zeros((1, num_steps, encoding_dim)))\n",
    "P = pos_encoding.P[:, :X.shape[1], :]\n",
    "d2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n",
    "         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in d2l.arange(6, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(f'{i} in binary is {i:>03b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P[0, :, :].unsqueeze(0).unsqueeze(0)\n",
    "d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n",
    "                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7 The Transformer Architecture\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: The Transformer uses stacked encoder-decoder blocks with self-attention and cross-attention. The encoder's self-attention sees all tokens; the decoder's self-attention is causal (masked) to preserve autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):  #@save\n",
    "    \"\"\"The positionwise feed-forward network.\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = PositionWiseFFN(4, 8)\n",
    "ffn.eval()\n",
    "ffn(d2l.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.LazyBatchNorm1d()\n",
    "X = d2l.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n",
    "# Compute mean and variance from X in the training mode\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):  #@save\n",
    "    \"\"\"The residual connection followed by layer normalization.\"\"\"\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_norm = AddNorm(4, 0.5)\n",
    "shape = (2, 3, 4)\n",
    "d2l.check_shape(add_norm(d2l.ones(shape), d2l.ones(shape)), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):  #@save\n",
    "    \"\"\"The Transformer encoder block.\"\"\"\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n",
    "                 use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d2l.ones((2, 100, 24))\n",
    "valid_lens = d2l.tensor([3, 2])\n",
    "encoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)\n",
    "encoder_blk.eval()\n",
    "d2l.check_shape(encoder_blk(X, valid_lens), X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):  #@save\n",
    "    \"\"\"The Transformer encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # Since positional encoding values are between -1 and 1, the embedding\n",
    "        # values are multiplied by the square root of the embedding dimension\n",
    "        # to rescale before they are summed up\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\n",
    "d2l.check_shape(encoder(d2l.ones((2, 100), dtype=torch.long), valid_lens),\n",
    "                (2, 100, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    # The i-th block in the Transformer decoder\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                 dropout)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                 dropout)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # During training, all the tokens of any output sequence are processed\n",
    "        # at the same time, so state[2][self.i] is None as initialized. When\n",
    "        # decoding any output sequence token by token during prediction,\n",
    "        # state[2][self.i] contains representations of the decoded output at\n",
    "        # the i-th block up to the current time step\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n",
    "            # row is [1, 2, ..., num_steps]\n",
    "            dec_valid_lens = torch.arange(\n",
    "                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        # Self-attention\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # Encoder-decoder attention. Shape of enc_outputs:\n",
    "        # (batch_size, num_steps, num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\n",
    "X = d2l.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "d2l.check_shape(decoder_blk(X, state)[0], X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            # Decoder self-attention weights\n",
    "            self._attention_weights[0][\n",
    "                i] = blk.attention1.attention.attention_weights\n",
    "            # Encoder-decoder attention weights\n",
    "            self._attention_weights[1][\n",
    "                i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.MTFraEng(batch_size=128)\n",
    "num_hiddens, num_blks, dropout = 256, 2, 0.2\n",
    "ffn_num_hiddens, num_heads = 64, 4\n",
    "encoder = TransformerEncoder(\n",
    "    len(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n",
    "    num_blks, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n",
    "    num_blks, dropout)\n",
    "model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                    lr=0.001)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "preds, _ = model.predict_step(\n",
    "    data.build(engs, fras), d2l.try_gpu(), data.num_steps)\n",
    "for en, fr, p in zip(engs, fras, preds):\n",
    "    translation = []\n",
    "    for token in data.tgt_vocab.to_tokens(p):\n",
    "        if token == '<eos>':\n",
    "            break\n",
    "        translation.append(token)\n",
    "    print(f'{en} => {translation}, bleu,'\n",
    "          f'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dec_attention_weights = model.predict_step(\n",
    "    data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\n",
    "enc_attention_weights = d2l.concat(model.encoder.attention_weights, 0)\n",
    "shape = (num_blks, num_heads, -1, data.num_steps)\n",
    "enc_attention_weights = d2l.reshape(enc_attention_weights, shape)\n",
    "d2l.check_shape(enc_attention_weights,\n",
    "                (num_blks, num_heads, data.num_steps, data.num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(\n",
    "    enc_attention_weights.cpu(), xlabel='Key positions',\n",
    "    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n",
    "    figsize=(7, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_attention_weights_2d = [head[0].tolist()\n",
    "                            for step in dec_attention_weights\n",
    "                            for attn in step for blk in attn for head in blk]\n",
    "dec_attention_weights_filled = d2l.tensor(\n",
    "    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\n",
    "shape = (-1, 2, num_blks, num_heads, data.num_steps)\n",
    "dec_attention_weights = d2l.reshape(dec_attention_weights_filled, shape)\n",
    "dec_self_attention_weights, dec_inter_attention_weights = \\\n",
    "    dec_attention_weights.permute(1, 2, 3, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.check_shape(dec_self_attention_weights,\n",
    "                (num_blks, num_heads, data.num_steps, data.num_steps))\n",
    "d2l.check_shape(dec_inter_attention_weights,\n",
    "                (num_blks, num_heads, data.num_steps, data.num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(\n",
    "    dec_self_attention_weights[:, :, :, :],\n",
    "    xlabel='Key positions', ylabel='Query positions',\n",
    "    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(\n",
    "    dec_inter_attention_weights, xlabel='Key positions',\n",
    "    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n",
    "    figsize=(7, 3.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8 Transformers for Vision (Vision Transformer - ViT)\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Vision Transformers split images into patches, treat them like tokens, and apply standard Transformer encoder. When trained on large datasets, ViTs outperform CNNs, showing Transformers' superior scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=96, patch_size=16, num_hiddens=512):\n",
    "        super().__init__()\n",
    "        def _make_tuple(x):\n",
    "            if not isinstance(x, (list, tuple)):\n",
    "                return (x, x)\n",
    "            return x\n",
    "        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (\n",
    "            img_size[1] // patch_size[1])\n",
    "        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,\n",
    "                                  stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Output shape: (batch size, no. of patches, no. of channels)\n",
    "        return self.conv(X).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4\n",
    "patch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)\n",
    "X = d2l.zeros(batch_size, 3, img_size, img_size)\n",
    "d2l.check_shape(patch_emb(X),\n",
    "                (batch_size, (img_size//patch_size)**2, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(mlp_num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.dense2(self.dropout1(self.gelu(\n",
    "            self.dense1(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,\n",
    "                 num_heads, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(norm_shape)\n",
    "        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.ln2 = nn.LayerNorm(norm_shape)\n",
    "        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        X = X + self.attention(*([self.ln1(X)] * 3), valid_lens)\n",
    "        return X + self.mlp(self.ln2(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d2l.ones((2, 100, 24))\n",
    "encoder_blk = ViTBlock(24, 24, 48, 8, 0.5)\n",
    "encoder_blk.eval()\n",
    "d2l.check_shape(encoder_blk(X), X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(d2l.Classifier):\n",
    "    \"\"\"Vision Transformer.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,\n",
    "                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,\n",
    "                 use_bias=False, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            img_size, patch_size, num_hiddens)\n",
    "        self.cls_token = nn.Parameter(d2l.zeros(1, 1, num_hiddens))\n",
    "        num_steps = self.patch_embedding.num_patches + 1  # Add the cls token\n",
    "        # Positional embeddings are learnable\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_steps, num_hiddens))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", ViTBlock(\n",
    "                num_hiddens, num_hiddens, mlp_num_hiddens,\n",
    "                num_heads, blk_dropout, use_bias))\n",
    "        self.head = nn.Sequential(nn.LayerNorm(num_hiddens),\n",
    "                                  nn.Linear(num_hiddens, num_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patch_embedding(X)\n",
    "        X = d2l.concat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n",
    "        X = self.dropout(X + self.pos_embedding)\n",
    "        for blk in self.blks:\n",
    "            X = blk(X)\n",
    "        return self.head(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size, patch_size = 96, 16\n",
    "num_hiddens, mlp_num_hiddens, num_heads, num_blks = 512, 2048, 8, 2\n",
    "emb_dropout, blk_dropout, lr = 0.1, 0.1, 0.1\n",
    "model = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,\n",
    "            num_blks, emb_dropout, blk_dropout, lr)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(img_size, img_size))\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9 Large-Scale Pretraining with Transformers\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Transformers exhibit power-law scaling - performance improves predictably with more parameters, data, and compute. This has driven the development of ever-larger models like BERT (encoder-only), T5 (encoder-decoder), and GPT series (decoder-only).\n",
    "\n",
    "### Key Model Architectures:\n",
    "\n",
    "- **Encoder-only (BERT)**: Bidirectional attention, good for understanding tasks (classification, NER)\n",
    "- **Encoder-Decoder (T5)**: Full architecture, good for seq2seq tasks (translation, summarization)\n",
    "- **Decoder-only (GPT)**: Causal attention, good for generation (text completion, chat)\n",
    "\n",
    "### Scaling Laws:\n",
    "\n",
    "- Performance scales as power law with model size, dataset size, and compute\n",
    "- Larger models are more sample-efficient\n",
    "- Led to models like GPT-3 (175B params), PaLM (540B params), and beyond\n",
    "\n",
    "### In-Context Learning:\n",
    "\n",
    "- Zero-shot: Task description only\n",
    "- One-shot: One example\n",
    "- Few-shot: Multiple examples\n",
    "- Chain-of-thought prompting for complex reasoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
