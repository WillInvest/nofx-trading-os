{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Modern Recurrent Neural Networks\n",
    "\n",
    "This chapter introduces the key ideas behind the most successful RNN architectures for sequences, which stem from two foundational papers:\n",
    "- **Long Short-Term Memory (LSTM)** - introduces memory cells to overcome vanishing gradients\n",
    "- **Bidirectional RNNs** - uses information from both future and past time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.1 Long Short-Term Memory (LSTM)\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: LSTMs address vanishing gradients by introducing a **memory cell** with an internal state that has a self-connected recurrent edge of fixed weight 1. This ensures gradients can pass across many time steps without vanishing or exploding. The term \"long short-term memory\" refers to an intermediate type of storage between long-term (weights) and short-term (activations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Memory Cell\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: LSTMs have **three gates** controlling information flow:\n",
    "- **Input gate** ($\\mathbf{I}_t$): determines how much new data to incorporate\n",
    "- **Forget gate** ($\\mathbf{F}_t$): determines whether to keep or flush the internal state\n",
    "- **Output gate** ($\\mathbf{O}_t$): determines whether the memory cell should influence the output\n",
    "\n",
    "All gates use sigmoid activation (values in $(0, 1)$), while the **input node** uses tanh (values in $(-1, 1)$).\n",
    "\n",
    "The memory cell update:\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$\n",
    "\n",
    "The hidden state:\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(d2l.zeros(num_hiddens)))\n",
    "\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LSTMScratch)\n",
    "def forward(self, inputs, H_C=None):\n",
    "    if H_C is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "        C = d2l.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "    else:\n",
    "        H, C = H_C\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = d2l.sigmoid(d2l.matmul(X, self.W_xi) +\n",
    "                        d2l.matmul(H, self.W_hi) + self.b_i)\n",
    "        F = d2l.sigmoid(d2l.matmul(X, self.W_xf) +\n",
    "                        d2l.matmul(H, self.W_hf) + self.b_f)\n",
    "        O = d2l.sigmoid(d2l.matmul(X, self.W_xo) +\n",
    "                        d2l.matmul(H, self.W_ho) + self.b_o)\n",
    "        C_tilde = d2l.tanh(d2l.matmul(X, self.W_xc) +\n",
    "                           d2l.matmul(H, self.W_hc) + self.b_c)\n",
    "        C = F * C + I * C_tilde\n",
    "        H = O * d2l.tanh(C)\n",
    "        outputs.append(H)\n",
    "    return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        return self.rnn(inputs, H_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.2 Gated Recurrent Units (GRU)\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: GRUs are a streamlined version of LSTMs that often achieve comparable performance but are **faster to compute**. GRUs replace LSTM's three gates with just **two gates**: reset gate and update gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Gate and Update Gate\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: \n",
    "- **Reset gate** ($\\mathbf{R}_t$): Controls how much of the previous state to remember (captures short-term dependencies)\n",
    "- **Update gate** ($\\mathbf{Z}_t$): Controls how much of the new state is just a copy of the old one (captures long-term dependencies)\n",
    "\n",
    "The hidden state update uses convex combinations:\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(d2l.zeros(num_hiddens)))\n",
    "            \n",
    "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(GRUScratch)\n",
    "def forward(self, inputs, H=None):\n",
    "    if H is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) +\n",
    "                        d2l.matmul(H, self.W_hz) + self.b_z)\n",
    "        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) + \n",
    "                        d2l.matmul(H, self.W_hr) + self.b_r)\n",
    "        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) + \n",
    "                           d2l.matmul(R * H, self.W_hh) + self.b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilde\n",
    "        outputs.append(H)\n",
    "    return outputs, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\n",
    "trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.3 Deep Recurrent Neural Networks\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Deep RNNs stack multiple RNN layers on top of each other. The output of one RNN layer becomes the input to the next layer. Each hidden state depends on both the **same layer's previous time step** and the **previous layer's same time step**. Common depths are 1-8 layers, with widths of 64-2056 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnns = nn.Sequential(*[d2l.RNNScratch(\n",
    "            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)\n",
    "                                    for i in range(num_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(StackedRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    outputs = inputs\n",
    "    if Hs is None: Hs = [None] * self.num_layers\n",
    "    for i in range(self.num_layers):\n",
    "        outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n",
    "        outputs = d2l.stack(outputs, 0)\n",
    "    return outputs, Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n",
    "                              num_hiddens=32, num_layers=2)\n",
    "model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n",
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(d2l.RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\n",
    "                          dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\n",
    "model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.4 Bidirectional Recurrent Neural Networks\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Bidirectional RNNs condition predictions at each time step on **both leftward and rightward context**. This is achieved by implementing two unidirectional RNN layers chained in opposite directions, then concatenating their outputs. The output dimension doubles: $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$.\n",
    "\n",
    "Example: For predicting a masked word, context from both directions matters:\n",
    "- \"I am `___`.\" â†’ \"happy\" seems likely\n",
    "- \"I am `___` hungry.\" â†’ \"not\" or \"very\" seem plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(BiRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "    f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "    outputs = [d2l.concat((f, b), -1) for f, b in zip(\n",
    "        f_outputs, reversed(b_outputs))]\n",
    "    return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.5 Machine Translation and the Dataset\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Machine translation is a **sequence-to-sequence** (seq2seq) problem where inputs and outputs may have different lengths and corresponding words may not occur in the same order. Key concepts:\n",
    "- **<eos>** token: marks end of sequence\n",
    "- **<pad>** token: pads shorter sequences to uniform length\n",
    "- **<bos>** token: beginning-of-sequence for decoder input\n",
    "- **<unk>** token: replaces infrequent tokens (freq < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTFraEng(d2l.DataModule):  #@save\n",
    "    \"\"\"The English-French dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root, \n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MTFraEng() \n",
    "raw_text = data._download()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _preprocess(self, text):\n",
    "    # Replace non-breaking space with space\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    # Insert space between words and punctuation marks\n",
    "    no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data._preprocess(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _tokenize(self, text, max_examples=None):\n",
    "    src, tgt = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if max_examples and i > max_examples: break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            # Skip empty tokens\n",
    "            src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "            tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "    return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = data._tokenize(text)\n",
    "src[:6], tgt[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
    "    \"\"\"Plot the histogram for list length pairs.\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    _, _, patches = d2l.plt.hist(\n",
    "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
    "    d2l.plt.xlabel(xlabel)\n",
    "    d2l.plt.ylabel(ylabel)\n",
    "    for patch in patches[1].patches:\n",
    "        patch.set_hatch('/')\n",
    "    d2l.plt.legend(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',\n",
    "                        'count', src, tgt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Sequences of Fixed Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "    super(MTFraEng, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "        self._download())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "    def _build_array(sentences, vocab, is_tgt=False):\n",
    "        pad_or_trim = lambda seq, t: (\n",
    "            seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "        sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "        if is_tgt:\n",
    "            sentences = [['<bos>'] + s for s in sentences]\n",
    "        if vocab is None:\n",
    "            vocab = d2l.Vocab(sentences, min_freq=2)\n",
    "        array = d2l.tensor([vocab[s] for s in sentences])\n",
    "        valid_len = d2l.reduce_sum(\n",
    "            d2l.astype(array != vocab['<pad>'], d2l.int32), 1)\n",
    "        return array, vocab, valid_len\n",
    "    src, tgt = self._tokenize(self._preprocess(raw_text), \n",
    "                              self.num_train + self.num_val)\n",
    "    src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "    tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "    return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "            src_vocab, tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader(self.arrays, train, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MTFraEng(batch_size=3)\n",
    "src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\n",
    "print('source:', d2l.astype(src, d2l.int32))\n",
    "print('decoder input:', d2l.astype(tgt, d2l.int32))\n",
    "print('source len excluding pad:', d2l.astype(src_valid_len, d2l.int32))\n",
    "print('label:', d2l.astype(label, d2l.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)  #@save\n",
    "def build(self, src_sentences, tgt_sentences):\n",
    "    raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "        src_sentences, tgt_sentences)])\n",
    "    arrays, _, _ = self._build_arrays(\n",
    "        raw_text, self.src_vocab, self.tgt_vocab)\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt, _,  _ = data.build(['hi .'], ['salut .'])\n",
    "print('source:', data.src_vocab.to_tokens(d2l.astype(src[0], d2l.int32)))\n",
    "print('target:', data.tgt_vocab.to_tokens(d2l.astype(tgt[0], d2l.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.6 The Encoder-Decoder Architecture\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: The encoder-decoder architecture handles **variable-length inputs and outputs**:\n",
    "- **Encoder**: Takes variable-length sequence as input, transforms it into a fixed-shape **context variable** (state)\n",
    "- **Decoder**: Acts as a conditional language model, taking the encoded state and generating output tokens one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  #@save\n",
    "    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):  #@save\n",
    "    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the Encoder and Decoder Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(d2l.Classifier):  #@save\n",
    "    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.7 Sequence-to-Sequence Learning for Machine Translation\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: The seq2seq model uses:\n",
    "- **Teacher forcing** during training: feed the ground truth target sequence to the decoder\n",
    "- **Embedding layer**: converts token indices to dense feature vectors\n",
    "- **Context concatenation**: encoder's final hidden state is concatenated with decoder input at all time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq2seq(module):  #@save\n",
    "    \"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "         nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):  #@save\n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "            \n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int64))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 9\n",
    "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "X = d2l.zeros((batch_size, num_steps))\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "\n",
    "d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,\n",
    "                           num_layers, dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(init_seq2seq)\n",
    "            \n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = d2l.concat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "state = decoder.init_state(encoder(X))\n",
    "dec_outputs, state = decoder(X, state)\n",
    "\n",
    "d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\n",
    "d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder for Sequence-to-Sequence Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(d2l.EncoderDecoder):  #@save\n",
    "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function with Masking\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Padding tokens should be excluded from loss calculations. We mask irrelevant entries with zero values so that multiplication of any irrelevant prediction with zero equates to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Seq2Seq)\n",
    "def loss(self, Y_hat, Y):\n",
    "    l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n",
    "    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n",
    "    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.MTFraEng(batch_size=128) \n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                lr=0.005)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.EncoderDecoder)  #@save\n",
    "def predict_step(self, batch, device, num_steps,\n",
    "                 save_attention_weights=False):\n",
    "    batch = [d2l.to(a, device) for a in batch]\n",
    "    src, tgt, src_valid_len, _ = batch\n",
    "    enc_all_outputs = self.encoder(src, src_valid_len)\n",
    "    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
    "    outputs, attention_weights = [d2l.expand_dims(tgt[:, 0], 1), ], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = self.decoder(outputs[-1], dec_state)\n",
    "        outputs.append(d2l.argmax(Y, 2))\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weights.append(self.decoder.attention_weights)\n",
    "    return d2l.concat(outputs[1:], 1), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Predicted Sequences (BLEU)\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: **BLEU (Bilingual Evaluation Understudy)** measures sequence quality by evaluating whether $n$-grams in the predicted sequence appear in the target sequence. The score:\n",
    "- Ranges from 0 to 1 (1 = perfect match)\n",
    "- Penalizes shorter predicted sequences\n",
    "- Weights longer $n$-gram matches more heavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"Compute the BLEU.\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, min(k, len_pred) + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "preds, _ = model.predict_step(\n",
    "    data.build(engs, fras), d2l.try_gpu(), data.num_steps)\n",
    "for en, fr, p in zip(engs, fras, preds):\n",
    "    translation = []\n",
    "    for token in data.tgt_vocab.to_tokens(p):\n",
    "        if token == '<eos>':\n",
    "            break\n",
    "        translation.append(token)        \n",
    "    print(f'{en} => {translation}, bleu,'\n",
    "          f'{bleu(\" \".join(translation), fr, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.8 Beam Search\n",
    "\n",
    "ðŸ”‘ **KEY INSIGHT**: Decoding strategies form a spectrum:\n",
    "- **Greedy search**: $\\mathcal{O}(|\\mathcal{Y}|T')$ - fast but suboptimal, selects highest probability token at each step\n",
    "- **Exhaustive search**: $\\mathcal{O}(|\\mathcal{Y}|^{T'})$ - optimal but computationally infeasible\n",
    "- **Beam search**: $\\mathcal{O}(k|\\mathcal{Y}|T')$ - keeps top $k$ candidates (beam size), balances efficiency and quality\n",
    "\n",
    "Beam search scoring function (with length penalty):\n",
    "$$\\frac{1}{L^\\alpha} \\sum_{t'=1}^L \\log P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$$\n",
    "where $\\alpha$ is typically 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Key takeaways from Chapter 10:\n",
    "\n",
    "1. **LSTMs** use memory cells with input, forget, and output gates to address vanishing gradients\n",
    "2. **GRUs** are a streamlined alternative with reset and update gates, often comparable performance but faster\n",
    "3. **Deep RNNs** stack multiple layers, with each cell depending on same layer's previous step AND previous layer's same step\n",
    "4. **Bidirectional RNNs** use both forward and backward context for predictions\n",
    "5. **Encoder-Decoder** architecture handles variable-length input/output sequences\n",
    "6. **Teacher forcing** feeds ground truth (not predictions) to decoder during training\n",
    "7. **BLEU** score evaluates translation quality via n-gram matching\n",
    "8. **Beam search** provides a practical trade-off between greedy and exhaustive search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
